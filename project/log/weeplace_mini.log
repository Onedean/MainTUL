The 0 round, start training with random seed 666
Train epoch:0 batch:0 loss_stu_ce:5.992973 loss_tea_ce:6.680361 loss_dis:0.663889 loss_sum:19.312223 loss:32.987526
Train epoch:0 batch:0 loss_stu_ce:5.993982 loss_tea_ce:6.146419 loss_dis:0.153490 loss_sum:13.675303 loss:32.987526
Train epoch:0 batch:40 loss_stu_ce:5.339369 loss_tea_ce:5.942176 loss_dis:0.028024 loss_sum:11.561785 loss:23.670272
Train epoch:0 batch:40 loss_stu_ce:3.326374 loss_tea_ce:5.944949 loss_dis:0.093354 loss_sum:10.204862 loss:23.670272
Train epoch:0 batch:80 loss_stu_ce:4.815185 loss_tea_ce:5.904855 loss_dis:0.039907 loss_sum:11.119104 loss:22.134895
Train epoch:0 batch:80 loss_stu_ce:1.268702 loss_tea_ce:5.911122 loss_dis:0.136761 loss_sum:8.547433 loss:22.134895
Train epoch:0 batch:120 loss_stu_ce:4.312977 loss_tea_ce:4.032039 loss_dis:0.080734 loss_sum:9.152357 loss:21.032033
Train epoch:0 batch:120 loss_stu_ce:0.592929 loss_tea_ce:5.036752 loss_dis:0.155983 loss_sum:7.189516 loss:21.032033
Train epoch:0 batch:160 loss_stu_ce:3.833739 loss_tea_ce:1.292970 loss_dis:0.134063 loss_sum:6.467340 loss:19.384353
Train epoch:0 batch:160 loss_stu_ce:0.438967 loss_tea_ce:4.094455 loss_dis:0.151150 loss_sum:6.044918 loss:19.384353
Valid epoch:0 loss:5.861637 acc@1:0.448592 acc@5:0.572136 macro_p:0.633984 macro_r:0.416703 macro_f1:0.452394
Validation loss decreased (inf --> 5.861637).  Saving model ...
Train epoch:1 batch:0 loss_stu_ce:3.456588 loss_tea_ce:0.761494 loss_dis:0.127460 loss_sum:5.492682 loss:10.914606
Train epoch:1 batch:0 loss_stu_ce:0.383801 loss_tea_ce:3.697321 loss_dis:0.134080 loss_sum:5.421924 loss:10.914606
Train epoch:1 batch:40 loss_stu_ce:3.360422 loss_tea_ce:0.436000 loss_dis:0.133342 loss_sum:5.129837 loss:10.942972
Train epoch:1 batch:40 loss_stu_ce:0.308129 loss_tea_ce:3.581119 loss_dis:0.132009 loss_sum:5.209337 loss:10.942972
Train epoch:1 batch:80 loss_stu_ce:3.385931 loss_tea_ce:0.423525 loss_dis:0.126973 loss_sum:5.079191 loss:10.659962
Train epoch:1 batch:80 loss_stu_ce:0.285290 loss_tea_ce:3.537045 loss_dis:0.128445 loss_sum:5.106784 loss:10.659962
Train epoch:1 batch:120 loss_stu_ce:3.304951 loss_tea_ce:0.333472 loss_dis:0.128421 loss_sum:4.922638 loss:10.456187
Train epoch:1 batch:120 loss_stu_ce:0.285750 loss_tea_ce:3.569183 loss_dis:0.121148 loss_sum:5.066413 loss:10.456187
Train epoch:1 batch:160 loss_stu_ce:3.258247 loss_tea_ce:0.370745 loss_dis:0.120645 loss_sum:4.835441 loss:10.301724
Train epoch:1 batch:160 loss_stu_ce:0.312267 loss_tea_ce:3.492119 loss_dis:0.114519 loss_sum:4.949578 loss:10.301724
Valid epoch:1 loss:4.770514 acc@1:0.499628 acc@5:0.627550 macro_p:0.647391 macro_r:0.471188 macro_f1:0.506449
Validation loss decreased (5.861637 --> 4.770514).  Saving model ...
Train epoch:2 batch:0 loss_stu_ce:2.704026 loss_tea_ce:0.295955 loss_dis:0.109169 loss_sum:4.091669 loss:8.449235
Train epoch:2 batch:0 loss_stu_ce:0.247743 loss_tea_ce:3.008518 loss_dis:0.110131 loss_sum:4.357566 loss:8.449235
Train epoch:2 batch:40 loss_stu_ce:2.851649 loss_tea_ce:0.284990 loss_dis:0.121612 loss_sum:4.352760 loss:8.909563
Train epoch:2 batch:40 loss_stu_ce:0.269143 loss_tea_ce:3.137227 loss_dis:0.116763 loss_sum:4.573997 loss:8.909563
Train epoch:2 batch:80 loss_stu_ce:2.826934 loss_tea_ce:0.279739 loss_dis:0.119532 loss_sum:4.301990 loss:8.818874
Train epoch:2 batch:80 loss_stu_ce:0.239530 loss_tea_ce:3.169167 loss_dis:0.116806 loss_sum:4.576757 loss:8.818874
Train epoch:2 batch:120 loss_stu_ce:2.661668 loss_tea_ce:0.274668 loss_dis:0.113601 loss_sum:4.072347 loss:8.753625
Train epoch:2 batch:120 loss_stu_ce:0.251069 loss_tea_ce:3.013171 loss_dis:0.112222 loss_sum:4.386462 loss:8.753625
Train epoch:2 batch:160 loss_stu_ce:2.874106 loss_tea_ce:0.248271 loss_dis:0.119646 loss_sum:4.318839 loss:8.715105
Train epoch:2 batch:160 loss_stu_ce:0.228894 loss_tea_ce:3.204448 loss_dis:0.114338 loss_sum:4.576717 loss:8.715105
Valid epoch:2 loss:4.473245 acc@1:0.522256 acc@5:0.660666 macro_p:0.635978 macro_r:0.495910 macro_f1:0.525286
Validation loss decreased (4.770514 --> 4.473245).  Saving model ...
Train epoch:3 batch:0 loss_stu_ce:2.807976 loss_tea_ce:0.275733 loss_dis:0.117549 loss_sum:4.259203 loss:8.891150
Train epoch:3 batch:0 loss_stu_ce:0.221053 loss_tea_ce:3.248799 loss_dis:0.116210 loss_sum:4.631947 loss:8.891150
Train epoch:3 batch:40 loss_stu_ce:2.421510 loss_tea_ce:0.228094 loss_dis:0.115600 loss_sum:3.805606 loss:7.843032
Train epoch:3 batch:40 loss_stu_ce:0.207640 loss_tea_ce:2.862575 loss_dis:0.113597 loss_sum:4.206183 loss:7.843032
Train epoch:3 batch:80 loss_stu_ce:2.467400 loss_tea_ce:0.206425 loss_dis:0.120757 loss_sum:3.881392 loss:7.805192
Train epoch:3 batch:80 loss_stu_ce:0.212390 loss_tea_ce:2.855659 loss_dis:0.117416 loss_sum:4.242206 loss:7.805192
Train epoch:3 batch:120 loss_stu_ce:2.295453 loss_tea_ce:0.219608 loss_dis:0.112842 loss_sum:3.643477 loss:7.772724
Train epoch:3 batch:120 loss_stu_ce:0.210483 loss_tea_ce:2.622881 loss_dis:0.104391 loss_sum:3.877270 loss:7.772724
Train epoch:3 batch:160 loss_stu_ce:2.222222 loss_tea_ce:0.230149 loss_dis:0.112990 loss_sum:3.582268 loss:7.790200
Train epoch:3 batch:160 loss_stu_ce:0.221541 loss_tea_ce:2.663779 loss_dis:0.104082 loss_sum:3.926136 loss:7.790200
Valid epoch:3 loss:4.345684 acc@1:0.534066 acc@5:0.674705 macro_p:0.623340 macro_r:0.509982 macro_f1:0.534408
Validation loss decreased (4.473245 --> 4.345684).  Saving model ...
Train epoch:4 batch:0 loss_stu_ce:2.093364 loss_tea_ce:0.221578 loss_dis:0.100329 loss_sum:3.318237 loss:7.005619
Train epoch:4 batch:0 loss_stu_ce:0.205173 loss_tea_ce:2.518093 loss_dis:0.096412 loss_sum:3.687382 loss:7.005619
Train epoch:4 batch:40 loss_stu_ce:2.000762 loss_tea_ce:0.180416 loss_dis:0.108311 loss_sum:3.264285 loss:7.025058
Train epoch:4 batch:40 loss_stu_ce:0.177822 loss_tea_ce:2.384393 loss_dis:0.108811 loss_sum:3.650324 loss:7.025058
Train epoch:4 batch:80 loss_stu_ce:2.000887 loss_tea_ce:0.208489 loss_dis:0.111037 loss_sum:3.319748 loss:7.131702
Train epoch:4 batch:80 loss_stu_ce:0.200173 loss_tea_ce:2.407526 loss_dis:0.101710 loss_sum:3.624799 loss:7.131702
Train epoch:4 batch:120 loss_stu_ce:2.175457 loss_tea_ce:0.200307 loss_dis:0.116275 loss_sum:3.538509 loss:7.147297
Train epoch:4 batch:120 loss_stu_ce:0.212277 loss_tea_ce:2.566127 loss_dis:0.108921 loss_sum:3.867616 loss:7.147297
Train epoch:4 batch:160 loss_stu_ce:1.799988 loss_tea_ce:0.193137 loss_dis:0.100197 loss_sum:2.995097 loss:7.146118
Train epoch:4 batch:160 loss_stu_ce:0.173683 loss_tea_ce:2.207221 loss_dis:0.088983 loss_sum:3.270736 loss:7.146118
Valid epoch:4 loss:4.299570 acc@1:0.542159 acc@5:0.685523 macro_p:0.609956 macro_r:0.517720 macro_f1:0.538957
Validation loss decreased (4.345684 --> 4.299570).  Saving model ...
Train epoch:5 batch:0 loss_stu_ce:1.544136 loss_tea_ce:0.186424 loss_dis:0.092810 loss_sum:2.658665 loss:5.940430
Train epoch:5 batch:0 loss_stu_ce:0.182592 loss_tea_ce:2.172391 loss_dis:0.092678 loss_sum:3.281765 loss:5.940430
Train epoch:5 batch:40 loss_stu_ce:1.740045 loss_tea_ce:0.156001 loss_dis:0.108713 loss_sum:2.983174 loss:6.384421
Train epoch:5 batch:40 loss_stu_ce:0.161272 loss_tea_ce:2.280935 loss_dis:0.101655 loss_sum:3.458759 loss:6.384421
Train epoch:5 batch:80 loss_stu_ce:1.898477 loss_tea_ce:0.168896 loss_dis:0.110025 loss_sum:3.167621 loss:6.448479
Train epoch:5 batch:80 loss_stu_ce:0.173735 loss_tea_ce:2.427426 loss_dis:0.095691 loss_sum:3.558075 loss:6.448479
Train epoch:5 batch:120 loss_stu_ce:2.018396 loss_tea_ce:0.171385 loss_dis:0.109256 loss_sum:3.282340 loss:6.507863
Train epoch:5 batch:120 loss_stu_ce:0.168072 loss_tea_ce:2.638637 loss_dis:0.104582 loss_sum:3.852532 loss:6.507863
Train epoch:5 batch:160 loss_stu_ce:1.898726 loss_tea_ce:0.162992 loss_dis:0.107601 loss_sum:3.137728 loss:6.523757
Train epoch:5 batch:160 loss_stu_ce:0.179035 loss_tea_ce:2.438268 loss_dis:0.100043 loss_sum:3.617731 loss:6.523757
Valid epoch:5 loss:4.251285 acc@1:0.551656 acc@5:0.690726 macro_p:0.611664 macro_r:0.528916 macro_f1:0.548549
Validation loss decreased (4.299570 --> 4.251285).  Saving model ...
Train epoch:6 batch:0 loss_stu_ce:1.520288 loss_tea_ce:0.173485 loss_dis:0.093369 loss_sum:2.627464 loss:5.602410
Train epoch:6 batch:0 loss_stu_ce:0.176203 loss_tea_ce:1.969395 loss_dis:0.082935 loss_sum:2.974946 loss:5.602410
Train epoch:6 batch:40 loss_stu_ce:1.674433 loss_tea_ce:0.166182 loss_dis:0.106073 loss_sum:2.901346 loss:5.924303
Train epoch:6 batch:40 loss_stu_ce:0.167375 loss_tea_ce:2.104023 loss_dis:0.092874 loss_sum:3.200138 loss:5.924303
Train epoch:6 batch:80 loss_stu_ce:1.650877 loss_tea_ce:0.145701 loss_dis:0.109237 loss_sum:2.888947 loss:5.970409
Train epoch:6 batch:80 loss_stu_ce:0.163891 loss_tea_ce:2.120300 loss_dis:0.094780 loss_sum:3.231987 loss:5.970409
Train epoch:6 batch:120 loss_stu_ce:1.722638 loss_tea_ce:0.154637 loss_dis:0.110335 loss_sum:2.980625 loss:6.049337
Train epoch:6 batch:120 loss_stu_ce:0.157962 loss_tea_ce:2.351515 loss_dis:0.099852 loss_sum:3.508001 loss:6.049337
Train epoch:6 batch:160 loss_stu_ce:1.587808 loss_tea_ce:0.150840 loss_dis:0.099509 loss_sum:2.733738 loss:6.109541
Train epoch:6 batch:160 loss_stu_ce:0.171367 loss_tea_ce:2.171698 loss_dis:0.090933 loss_sum:3.252399 loss:6.109541
Valid epoch:6 loss:4.251116 acc@1:0.553142 acc@5:0.692047 macro_p:0.607308 macro_r:0.530612 macro_f1:0.549020
Validation loss decreased (4.251285 --> 4.251116).  Saving model ...
Train epoch:7 batch:0 loss_stu_ce:1.426287 loss_tea_ce:0.172360 loss_dis:0.097388 loss_sum:2.572526 loss:5.699712
Train epoch:7 batch:0 loss_stu_ce:0.172135 loss_tea_ce:2.080710 loss_dis:0.087434 loss_sum:3.127187 loss:5.699712
Train epoch:7 batch:40 loss_stu_ce:1.395239 loss_tea_ce:0.138125 loss_dis:0.100820 loss_sum:2.541566 loss:5.555851
Train epoch:7 batch:40 loss_stu_ce:0.156568 loss_tea_ce:1.951675 loss_dis:0.086064 loss_sum:2.968877 loss:5.555851
Train epoch:7 batch:80 loss_stu_ce:1.612576 loss_tea_ce:0.135783 loss_dis:0.106152 loss_sum:2.809877 loss:5.649970
Train epoch:7 batch:80 loss_stu_ce:0.148727 loss_tea_ce:2.280969 loss_dis:0.099108 loss_sum:3.420774 loss:5.649970
Train epoch:7 batch:120 loss_stu_ce:1.509453 loss_tea_ce:0.147825 loss_dis:0.109638 loss_sum:2.753657 loss:5.715240
Train epoch:7 batch:120 loss_stu_ce:0.162620 loss_tea_ce:2.308192 loss_dis:0.095351 loss_sum:3.424324 loss:5.715240
Train epoch:7 batch:160 loss_stu_ce:1.562744 loss_tea_ce:0.161260 loss_dis:0.107178 loss_sum:2.795788 loss:5.772411
Train epoch:7 batch:160 loss_stu_ce:0.180587 loss_tea_ce:2.130494 loss_dis:0.087276 loss_sum:3.183838 loss:5.772411
Valid epoch:7 loss:4.268495 acc@1:0.554216 acc@5:0.694525 macro_p:0.609129 macro_r:0.532689 macro_f1:0.550932
EarlyStopping counter: 1 out of 3
Train epoch:8 batch:0 loss_stu_ce:1.246709 loss_tea_ce:0.144447 loss_dis:0.091898 loss_sum:2.310134 loss:4.798645
Train epoch:8 batch:0 loss_stu_ce:0.142481 loss_tea_ce:1.603503 loss_dis:0.074253 loss_sum:2.488511 loss:4.798645
Train epoch:8 batch:40 loss_stu_ce:1.256691 loss_tea_ce:0.124920 loss_dis:0.099132 loss_sum:2.372929 loss:5.349354
Train epoch:8 batch:40 loss_stu_ce:0.149394 loss_tea_ce:1.963992 loss_dis:0.089243 loss_sum:3.005816 loss:5.349354
Train epoch:8 batch:80 loss_stu_ce:1.451680 loss_tea_ce:0.146515 loss_dis:0.104368 loss_sum:2.641875 loss:5.425333
Train epoch:8 batch:80 loss_stu_ce:0.162602 loss_tea_ce:2.084047 loss_dis:0.089869 loss_sum:3.145338 loss:5.425333
Train epoch:8 batch:120 loss_stu_ce:1.544591 loss_tea_ce:0.123990 loss_dis:0.104612 loss_sum:2.714699 loss:5.454212
Train epoch:8 batch:120 loss_stu_ce:0.144811 loss_tea_ce:2.097488 loss_dis:0.091134 loss_sum:3.153636 loss:5.454212
Train epoch:8 batch:160 loss_stu_ce:1.280772 loss_tea_ce:0.137225 loss_dis:0.092747 loss_sum:2.345472 loss:5.477075
Train epoch:8 batch:160 loss_stu_ce:0.136501 loss_tea_ce:1.766208 loss_dis:0.080222 loss_sum:2.704927 loss:5.477075
Valid epoch:8 loss:4.287037 acc@1:0.555124 acc@5:0.695185 macro_p:0.600740 macro_r:0.534615 macro_f1:0.551163
EarlyStopping counter: 2 out of 3
Train epoch:9 batch:0 loss_stu_ce:1.044339 loss_tea_ce:0.131625 loss_dis:0.087388 loss_sum:2.049843 loss:4.521426
Train epoch:9 batch:0 loss_stu_ce:0.149446 loss_tea_ce:1.596680 loss_dis:0.072546 loss_sum:2.471583 loss:4.521426
Train epoch:9 batch:40 loss_stu_ce:1.081129 loss_tea_ce:0.123626 loss_dis:0.094105 loss_sum:2.145809 loss:5.058336
Train epoch:9 batch:40 loss_stu_ce:0.131718 loss_tea_ce:1.635502 loss_dis:0.079817 loss_sum:2.565387 loss:5.058336
Train epoch:9 batch:80 loss_stu_ce:1.168062 loss_tea_ce:0.120016 loss_dis:0.096864 loss_sum:2.256719 loss:5.165459
Train epoch:9 batch:80 loss_stu_ce:0.142960 loss_tea_ce:1.735428 loss_dis:0.079135 loss_sum:2.669742 loss:5.165459
Train epoch:9 batch:120 loss_stu_ce:1.326413 loss_tea_ce:0.131197 loss_dis:0.102444 loss_sum:2.482052 loss:5.195589
Train epoch:9 batch:120 loss_stu_ce:0.150989 loss_tea_ce:2.109051 loss_dis:0.095456 loss_sum:3.214595 loss:5.195589
Train epoch:9 batch:160 loss_stu_ce:1.207833 loss_tea_ce:0.118774 loss_dis:0.099676 loss_sum:2.323367 loss:5.239193
Train epoch:9 batch:160 loss_stu_ce:0.138103 loss_tea_ce:1.947005 loss_dis:0.085966 loss_sum:2.944771 loss:5.239193
Valid epoch:9 loss:4.289137 acc@1:0.554381 acc@5:0.693864 macro_p:0.597464 macro_r:0.533546 macro_f1:0.548847
EarlyStopping counter: 3 out of 3
Early Stop!
Test 	 loss:3.205634 acc@1:0.451735 acc@5:0.582898 macro_p:0.513695 macro_r:0.437235 macro_f1:0.451231
Test 	 loss:3.416149 acc@1:0.411362 acc@5:0.524459 macro_p:0.481634 macro_r:0.397464 macro_f1:0.408486
Total time elapsed: 1144.5571s
Fininsh trainning in seed 666

