The 1 round, start training with random seed 666
Train epoch:0 batch:0 loss_stu_ce:6.684951 loss_tea_ce:7.193068 loss_dis:0.464249 loss_sum:18.520514 loss:33.634083
Train epoch:0 batch:0 loss_stu_ce:6.682610 loss_tea_ce:6.839493 loss_dis:0.159147 loss_sum:15.113570 loss:33.634083
Train epoch:0 batch:40 loss_stu_ce:6.024168 loss_tea_ce:6.632781 loss_dis:0.024564 loss_sum:12.902583 loss:26.426453
Train epoch:0 batch:40 loss_stu_ce:4.489916 loss_tea_ce:6.637837 loss_dis:0.074861 loss_sum:11.876363 loss:26.426453
Train epoch:0 batch:80 loss_stu_ce:5.297321 loss_tea_ce:5.609705 loss_dis:0.064731 loss_sum:11.554339 loss:24.899261
Train epoch:0 batch:80 loss_stu_ce:2.692312 loss_tea_ce:6.070411 loss_dis:0.141211 loss_sum:10.174833 loss:24.899261
Train epoch:0 batch:120 loss_stu_ce:4.814556 loss_tea_ce:3.020007 loss_dis:0.129724 loss_sum:9.131807 loss:23.009052
Train epoch:0 batch:120 loss_stu_ce:1.709582 loss_tea_ce:4.953590 loss_dis:0.168731 loss_sum:8.350479 loss:23.009052
Train epoch:0 batch:160 loss_stu_ce:4.313092 loss_tea_ce:1.990451 loss_dis:0.140896 loss_sum:7.712504 loss:21.273414
Train epoch:0 batch:160 loss_stu_ce:1.166921 loss_tea_ce:4.437243 loss_dis:0.161365 loss_sum:7.217809 loss:21.273414
Valid epoch:0 loss:7.190910 acc@1:0.402861 acc@5:0.534908 macro_p:0.604705 macro_r:0.382288 macro_f1:0.426617
Validation loss decreased (inf --> 7.190910).  Saving model ...
Train epoch:1 batch:0 loss_stu_ce:4.056882 loss_tea_ce:1.618682 loss_dis:0.133217 loss_sum:7.007737 loss:13.620852
Train epoch:1 batch:0 loss_stu_ce:0.928967 loss_tea_ce:4.214335 loss_dis:0.146981 loss_sum:6.613115 loss:13.620852
Train epoch:1 batch:40 loss_stu_ce:4.107192 loss_tea_ce:1.309234 loss_dis:0.144496 loss_sum:6.861391 loss:13.369166
Train epoch:1 batch:40 loss_stu_ce:0.760899 loss_tea_ce:4.282559 loss_dis:0.148976 loss_sum:6.533218 loss:13.369166
Train epoch:1 batch:80 loss_stu_ce:3.821015 loss_tea_ce:1.063372 loss_dis:0.138919 loss_sum:6.273578 loss:13.000601
Train epoch:1 batch:80 loss_stu_ce:0.589187 loss_tea_ce:4.016510 loss_dis:0.146457 loss_sum:6.070269 loss:13.000601
Train epoch:1 batch:120 loss_stu_ce:3.776764 loss_tea_ce:0.973976 loss_dis:0.141117 loss_sum:6.161907 loss:12.699134
Train epoch:1 batch:120 loss_stu_ce:0.552899 loss_tea_ce:3.935343 loss_dis:0.136756 loss_sum:5.855807 loss:12.699134
Train epoch:1 batch:160 loss_stu_ce:3.676291 loss_tea_ce:0.790083 loss_dis:0.141254 loss_sum:5.878913 loss:12.466837
Train epoch:1 batch:160 loss_stu_ce:0.442771 loss_tea_ce:3.867674 loss_dis:0.137597 loss_sum:5.686411 loss:12.466837
Valid epoch:1 loss:5.722919 acc@1:0.450416 acc@5:0.598696 macro_p:0.615372 macro_r:0.432292 macro_f1:0.472788
Validation loss decreased (7.190910 --> 5.722919).  Saving model ...
Train epoch:2 batch:0 loss_stu_ce:3.641752 loss_tea_ce:0.686615 loss_dis:0.136845 loss_sum:5.696820 loss:11.349195
Train epoch:2 batch:0 loss_stu_ce:0.410190 loss_tea_ce:3.929606 loss_dis:0.131258 loss_sum:5.652374 loss:11.349195
Train epoch:2 batch:40 loss_stu_ce:3.490930 loss_tea_ce:0.701830 loss_dis:0.137932 loss_sum:5.572080 loss:10.685879
Train epoch:2 batch:40 loss_stu_ce:0.399764 loss_tea_ce:3.684365 loss_dis:0.138514 loss_sum:5.469270 loss:10.685879
Train epoch:2 batch:80 loss_stu_ce:3.336405 loss_tea_ce:0.610776 loss_dis:0.132954 loss_sum:5.276718 loss:10.606630
Train epoch:2 batch:80 loss_stu_ce:0.370695 loss_tea_ce:3.590700 loss_dis:0.123366 loss_sum:5.195053 loss:10.606630
Train epoch:2 batch:120 loss_stu_ce:3.284082 loss_tea_ce:0.609967 loss_dis:0.132371 loss_sum:5.217755 loss:10.513799
Train epoch:2 batch:120 loss_stu_ce:0.374373 loss_tea_ce:3.541990 loss_dis:0.125689 loss_sum:5.173253 loss:10.513799
Train epoch:2 batch:160 loss_stu_ce:3.072154 loss_tea_ce:0.542601 loss_dis:0.129268 loss_sum:4.907430 loss:10.423131
Train epoch:2 batch:160 loss_stu_ce:0.335258 loss_tea_ce:3.314608 loss_dis:0.121769 loss_sum:4.867560 loss:10.423131
Valid epoch:2 loss:5.163667 acc@1:0.474603 acc@5:0.626901 macro_p:0.610496 macro_r:0.456059 macro_f1:0.492225
Validation loss decreased (5.722919 --> 5.163667).  Saving model ...
Train epoch:3 batch:0 loss_stu_ce:2.834909 loss_tea_ce:0.496158 loss_dis:0.119030 loss_sum:4.521365 loss:9.090712
Train epoch:3 batch:0 loss_stu_ce:0.272406 loss_tea_ce:3.153786 loss_dis:0.114315 loss_sum:4.569347 loss:9.090712
Train epoch:3 batch:40 loss_stu_ce:2.771174 loss_tea_ce:0.436705 loss_dis:0.126870 loss_sum:4.476576 loss:9.450756
Train epoch:3 batch:40 loss_stu_ce:0.271245 loss_tea_ce:2.998501 loss_dis:0.121779 loss_sum:4.487539 loss:9.450756
Train epoch:3 batch:80 loss_stu_ce:2.687208 loss_tea_ce:0.410934 loss_dis:0.123036 loss_sum:4.328503 loss:9.422629
Train epoch:3 batch:80 loss_stu_ce:0.275170 loss_tea_ce:2.953617 loss_dis:0.113693 loss_sum:4.365720 loss:9.422629
Train epoch:3 batch:120 loss_stu_ce:3.014030 loss_tea_ce:0.414948 loss_dis:0.127743 loss_sum:4.706403 loss:9.405537
Train epoch:3 batch:120 loss_stu_ce:0.279810 loss_tea_ce:3.322539 loss_dis:0.119570 loss_sum:4.798047 loss:9.405537
Train epoch:3 batch:160 loss_stu_ce:2.892011 loss_tea_ce:0.350465 loss_dis:0.126607 loss_sum:4.508542 loss:9.396619
Train epoch:3 batch:160 loss_stu_ce:0.270260 loss_tea_ce:3.126621 loss_dis:0.111223 loss_sum:4.509114 loss:9.396619
Valid epoch:3 loss:4.900043 acc@1:0.486779 acc@5:0.642643 macro_p:0.603119 macro_r:0.468443 macro_f1:0.501353
Validation loss decreased (5.163667 --> 4.900043).  Saving model ...
Train epoch:4 batch:0 loss_stu_ce:2.880760 loss_tea_ce:0.395509 loss_dis:0.119148 loss_sum:4.467748 loss:8.976090
Train epoch:4 batch:0 loss_stu_ce:0.278363 loss_tea_ce:3.138068 loss_dis:0.109191 loss_sum:4.508343 loss:8.976090
Train epoch:4 batch:40 loss_stu_ce:2.658456 loss_tea_ce:0.286581 loss_dis:0.128012 loss_sum:4.225157 loss:8.663498
Train epoch:4 batch:40 loss_stu_ce:0.222160 loss_tea_ce:2.959953 loss_dis:0.118952 loss_sum:4.371628 loss:8.663498
Train epoch:4 batch:80 loss_stu_ce:2.625483 loss_tea_ce:0.318931 loss_dis:0.122180 loss_sum:4.166218 loss:8.710407
Train epoch:4 batch:80 loss_stu_ce:0.225708 loss_tea_ce:2.869324 loss_dis:0.112446 loss_sum:4.219489 loss:8.710407
Train epoch:4 batch:120 loss_stu_ce:2.546700 loss_tea_ce:0.293297 loss_dis:0.122724 loss_sum:4.067234 loss:8.704644
Train epoch:4 batch:120 loss_stu_ce:0.236573 loss_tea_ce:2.840213 loss_dis:0.108011 loss_sum:4.156899 loss:8.704644
Train epoch:4 batch:160 loss_stu_ce:2.947942 loss_tea_ce:0.296476 loss_dis:0.129668 loss_sum:4.541095 loss:8.714863
Train epoch:4 batch:160 loss_stu_ce:0.248164 loss_tea_ce:3.385385 loss_dis:0.116398 loss_sum:4.797531 loss:8.714863
Valid epoch:4 loss:4.737056 acc@1:0.492969 acc@5:0.654040 macro_p:0.592688 macro_r:0.475446 macro_f1:0.503330
Validation loss decreased (4.900043 --> 4.737056).  Saving model ...
Train epoch:5 batch:0 loss_stu_ce:2.469048 loss_tea_ce:0.329522 loss_dis:0.116005 loss_sum:3.958619 loss:8.062628
Train epoch:5 batch:0 loss_stu_ce:0.223744 loss_tea_ce:2.839817 loss_dis:0.104045 loss_sum:4.104009 loss:8.062628
Train epoch:5 batch:40 loss_stu_ce:2.367813 loss_tea_ce:0.263790 loss_dis:0.121989 loss_sum:3.851497 loss:8.032015
Train epoch:5 batch:40 loss_stu_ce:0.191354 loss_tea_ce:2.773232 loss_dis:0.110714 loss_sum:4.071729 loss:8.032015
Train epoch:5 batch:80 loss_stu_ce:2.542236 loss_tea_ce:0.254105 loss_dis:0.124118 loss_sum:4.037520 loss:8.067935
Train epoch:5 batch:80 loss_stu_ce:0.202207 loss_tea_ce:2.828548 loss_dis:0.106938 loss_sum:4.100134 loss:8.067935
Train epoch:5 batch:120 loss_stu_ce:2.491624 loss_tea_ce:0.259757 loss_dis:0.121489 loss_sum:3.966270 loss:8.056241
Train epoch:5 batch:120 loss_stu_ce:0.199949 loss_tea_ce:2.852616 loss_dis:0.107472 loss_sum:4.127287 loss:8.056241
Train epoch:5 batch:160 loss_stu_ce:2.577429 loss_tea_ce:0.237334 loss_dis:0.122019 loss_sum:4.034950 loss:8.095544
Train epoch:5 batch:160 loss_stu_ce:0.197887 loss_tea_ce:2.921757 loss_dis:0.109678 loss_sum:4.216419 loss:8.095544
Valid epoch:5 loss:4.633250 acc@1:0.502070 acc@5:0.661583 macro_p:0.586936 macro_r:0.485864 macro_f1:0.510496
Validation loss decreased (4.737056 --> 4.633250).  Saving model ...
Train epoch:6 batch:0 loss_stu_ce:2.488935 loss_tea_ce:0.225914 loss_dis:0.119839 loss_sum:3.913241 loss:7.824181
Train epoch:6 batch:0 loss_stu_ce:0.190347 loss_tea_ce:2.705859 loss_dis:0.101473 loss_sum:3.910940 loss:7.824181
Train epoch:6 batch:40 loss_stu_ce:2.202670 loss_tea_ce:0.208421 loss_dis:0.120913 loss_sum:3.620224 loss:7.613705
Train epoch:6 batch:40 loss_stu_ce:0.174053 loss_tea_ce:2.496833 loss_dis:0.105262 loss_sum:3.723509 loss:7.613705
Train epoch:6 batch:80 loss_stu_ce:2.199257 loss_tea_ce:0.205192 loss_dis:0.119228 loss_sum:3.596729 loss:7.662985
Train epoch:6 batch:80 loss_stu_ce:0.165680 loss_tea_ce:2.608624 loss_dis:0.103596 loss_sum:3.810266 loss:7.662985
Train epoch:6 batch:120 loss_stu_ce:2.529319 loss_tea_ce:0.273991 loss_dis:0.126608 loss_sum:4.069394 loss:7.690046
Train epoch:6 batch:120 loss_stu_ce:0.227478 loss_tea_ce:2.900898 loss_dis:0.114170 loss_sum:4.270071 loss:7.690046
Train epoch:6 batch:160 loss_stu_ce:2.362989 loss_tea_ce:0.229243 loss_dis:0.123678 loss_sum:3.829016 loss:7.694330
Train epoch:6 batch:160 loss_stu_ce:0.189229 loss_tea_ce:2.705010 loss_dis:0.110235 loss_sum:3.996592 loss:7.694330
Valid epoch:6 loss:4.589379 acc@1:0.503915 acc@5:0.663838 macro_p:0.584691 macro_r:0.488997 macro_f1:0.511817
Validation loss decreased (4.633250 --> 4.589379).  Saving model ...
Train epoch:7 batch:0 loss_stu_ce:2.139500 loss_tea_ce:0.209161 loss_dis:0.114772 loss_sum:3.496383 loss:7.116339
Train epoch:7 batch:0 loss_stu_ce:0.187057 loss_tea_ce:2.466762 loss_dis:0.096614 loss_sum:3.619956 loss:7.116339
Train epoch:7 batch:40 loss_stu_ce:2.025334 loss_tea_ce:0.205181 loss_dis:0.117500 loss_sum:3.405514 loss:7.246423
Train epoch:7 batch:40 loss_stu_ce:0.162699 loss_tea_ce:2.418133 loss_dis:0.101393 loss_sum:3.594760 loss:7.246423
Train epoch:7 batch:80 loss_stu_ce:2.333782 loss_tea_ce:0.202774 loss_dis:0.125409 loss_sum:3.790648 loss:7.280581
Train epoch:7 batch:80 loss_stu_ce:0.169249 loss_tea_ce:2.618737 loss_dis:0.106703 loss_sum:3.855013 loss:7.280581
Train epoch:7 batch:120 loss_stu_ce:2.054921 loss_tea_ce:0.199495 loss_dis:0.118097 loss_sum:3.435387 loss:7.315327
Train epoch:7 batch:120 loss_stu_ce:0.168319 loss_tea_ce:2.482957 loss_dis:0.097458 loss_sum:3.625859 loss:7.315327
Train epoch:7 batch:160 loss_stu_ce:2.345501 loss_tea_ce:0.220434 loss_dis:0.120661 loss_sum:3.772547 loss:7.342463
Train epoch:7 batch:160 loss_stu_ce:0.186498 loss_tea_ce:2.701987 loss_dis:0.102736 loss_sum:3.915845 loss:7.342463
Valid epoch:7 loss:4.556576 acc@1:0.507728 acc@5:0.666544 macro_p:0.587894 macro_r:0.491848 macro_f1:0.516345
Validation loss decreased (4.589379 --> 4.556576).  Saving model ...
Train epoch:8 batch:0 loss_stu_ce:2.054081 loss_tea_ce:0.192843 loss_dis:0.114832 loss_sum:3.395247 loss:6.870951
Train epoch:8 batch:0 loss_stu_ce:0.156358 loss_tea_ce:2.394955 loss_dis:0.092439 loss_sum:3.475703 loss:6.870951
Train epoch:8 batch:40 loss_stu_ce:1.970519 loss_tea_ce:0.186062 loss_dis:0.120612 loss_sum:3.362699 loss:6.928963
Train epoch:8 batch:40 loss_stu_ce:0.155240 loss_tea_ce:2.351331 loss_dis:0.098891 loss_sum:3.495484 loss:6.928963
Train epoch:8 batch:80 loss_stu_ce:2.097531 loss_tea_ce:0.175545 loss_dis:0.120368 loss_sum:3.476757 loss:6.963307
Train epoch:8 batch:80 loss_stu_ce:0.150926 loss_tea_ce:2.430950 loss_dis:0.102605 loss_sum:3.607931 loss:6.963307
Train epoch:8 batch:120 loss_stu_ce:2.060869 loss_tea_ce:0.167862 loss_dis:0.118945 loss_sum:3.418183 loss:6.999316
Train epoch:8 batch:120 loss_stu_ce:0.142003 loss_tea_ce:2.460175 loss_dis:0.101692 loss_sum:3.619097 loss:6.999316
Train epoch:8 batch:160 loss_stu_ce:2.144801 loss_tea_ce:0.193860 loss_dis:0.117494 loss_sum:3.513600 loss:7.039237
Train epoch:8 batch:160 loss_stu_ce:0.159363 loss_tea_ce:2.533651 loss_dis:0.097880 loss_sum:3.671818 loss:7.039237
Valid epoch:8 loss:4.526167 acc@1:0.509367 acc@5:0.669618 macro_p:0.581739 macro_r:0.494069 macro_f1:0.515647
Validation loss decreased (4.556576 --> 4.526167).  Saving model ...
Train epoch:9 batch:0 loss_stu_ce:1.927633 loss_tea_ce:0.181493 loss_dis:0.113445 loss_sum:3.243576 loss:6.645188
Train epoch:9 batch:0 loss_stu_ce:0.158353 loss_tea_ce:2.311514 loss_dis:0.093175 loss_sum:3.401612 loss:6.645188
Train epoch:9 batch:40 loss_stu_ce:1.945206 loss_tea_ce:0.156050 loss_dis:0.118479 loss_sum:3.286051 loss:6.551929
Train epoch:9 batch:40 loss_stu_ce:0.136194 loss_tea_ce:2.426677 loss_dis:0.100468 loss_sum:3.567549 loss:6.551929
Train epoch:9 batch:80 loss_stu_ce:2.006752 loss_tea_ce:0.199484 loss_dis:0.121525 loss_sum:3.421483 loss:6.660143
Train epoch:9 batch:80 loss_stu_ce:0.144263 loss_tea_ce:2.379704 loss_dis:0.101542 loss_sum:3.539385 loss:6.660143
Train epoch:9 batch:120 loss_stu_ce:1.975822 loss_tea_ce:0.186712 loss_dis:0.118900 loss_sum:3.351536 loss:6.731063
Train epoch:9 batch:120 loss_stu_ce:0.163715 loss_tea_ce:2.355129 loss_dis:0.096295 loss_sum:3.481797 loss:6.731063
Train epoch:9 batch:160 loss_stu_ce:2.178998 loss_tea_ce:0.182890 loss_dis:0.121536 loss_sum:3.577244 loss:6.784874
Train epoch:9 batch:160 loss_stu_ce:0.151823 loss_tea_ce:2.577581 loss_dis:0.104418 loss_sum:3.773579 loss:6.784874
Valid epoch:9 loss:4.506480 acc@1:0.511991 acc@5:0.672898 macro_p:0.574967 macro_r:0.497717 macro_f1:0.516581
Validation loss decreased (4.526167 --> 4.506480).  Saving model ...
Train epoch:10 batch:0 loss_stu_ce:1.789882 loss_tea_ce:0.196395 loss_dis:0.111439 loss_sum:3.100665 loss:6.189802
Train epoch:10 batch:0 loss_stu_ce:0.157801 loss_tea_ce:2.068510 loss_dis:0.086283 loss_sum:3.089138 loss:6.189802
Train epoch:10 batch:40 loss_stu_ce:1.731132 loss_tea_ce:0.155497 loss_dis:0.115757 loss_sum:3.044201 loss:6.378661
Train epoch:10 batch:40 loss_stu_ce:0.136489 loss_tea_ce:2.126002 loss_dis:0.093401 loss_sum:3.196500 loss:6.378661
Train epoch:10 batch:80 loss_stu_ce:1.947856 loss_tea_ce:0.150945 loss_dis:0.118358 loss_sum:3.282378 loss:6.394117
Train epoch:10 batch:80 loss_stu_ce:0.136047 loss_tea_ce:2.300922 loss_dis:0.098683 loss_sum:3.423799 loss:6.394117
Train epoch:10 batch:120 loss_stu_ce:2.003025 loss_tea_ce:0.151328 loss_dis:0.123624 loss_sum:3.390597 loss:6.425754
Train epoch:10 batch:120 loss_stu_ce:0.156979 loss_tea_ce:2.441940 loss_dis:0.099542 loss_sum:3.594341 loss:6.425754
Train epoch:10 batch:160 loss_stu_ce:2.012928 loss_tea_ce:0.169686 loss_dis:0.118734 loss_sum:3.369957 loss:6.467135
Train epoch:10 batch:160 loss_stu_ce:0.148815 loss_tea_ce:2.491847 loss_dis:0.100768 loss_sum:3.648343 loss:6.467135
Valid epoch:10 loss:4.489048 acc@1:0.513467 acc@5:0.673841 macro_p:0.577638 macro_r:0.499243 macro_f1:0.519013
Validation loss decreased (4.506480 --> 4.489048).  Saving model ...
Train epoch:11 batch:0 loss_stu_ce:1.672940 loss_tea_ce:0.142106 loss_dis:0.110609 loss_sum:2.921134 loss:5.986609
Train epoch:11 batch:0 loss_stu_ce:0.143046 loss_tea_ce:2.039605 loss_dis:0.088283 loss_sum:3.065476 loss:5.986609
Train epoch:11 batch:40 loss_stu_ce:1.753824 loss_tea_ce:0.147954 loss_dis:0.116550 loss_sum:3.067276 loss:6.050232
Train epoch:11 batch:40 loss_stu_ce:0.129449 loss_tea_ce:2.215759 loss_dis:0.096086 loss_sum:3.306067 loss:6.050232
Train epoch:11 batch:80 loss_stu_ce:1.848372 loss_tea_ce:0.170491 loss_dis:0.116431 loss_sum:3.183169 loss:6.140009
Train epoch:11 batch:80 loss_stu_ce:0.133708 loss_tea_ce:2.207092 loss_dis:0.093339 loss_sum:3.274191 loss:6.140009
Train epoch:11 batch:120 loss_stu_ce:1.795328 loss_tea_ce:0.143188 loss_dis:0.118328 loss_sum:3.121801 loss:6.197335
Train epoch:11 batch:120 loss_stu_ce:0.149930 loss_tea_ce:2.134080 loss_dis:0.090438 loss_sum:3.188394 loss:6.197335
Train epoch:11 batch:160 loss_stu_ce:1.925696 loss_tea_ce:0.162577 loss_dis:0.117292 loss_sum:3.261189 loss:6.244539
Train epoch:11 batch:160 loss_stu_ce:0.126634 loss_tea_ce:2.248248 loss_dis:0.093189 loss_sum:3.306768 loss:6.244539
Valid epoch:11 loss:4.476394 acc@1:0.514369 acc@5:0.675317 macro_p:0.577040 macro_r:0.500103 macro_f1:0.519768
Validation loss decreased (4.489048 --> 4.476394).  Saving model ...
Train epoch:12 batch:0 loss_stu_ce:1.696781 loss_tea_ce:0.148233 loss_dis:0.111549 loss_sum:2.960507 loss:5.889079
Train epoch:12 batch:0 loss_stu_ce:0.126843 loss_tea_ce:1.955365 loss_dis:0.084636 loss_sum:2.928572 loss:5.889079

Train epoch:12 batch:40 loss_stu_ce:1.687662 loss_tea_ce:0.153627 loss_dis:0.113504 loss_sum:2.976325 loss:5.891047
Train epoch:12 batch:40 loss_stu_ce:0.136006 loss_tea_ce:2.094901 loss_dis:0.088757 loss_sum:3.118473 loss:5.891047
Train epoch:12 batch:80 loss_stu_ce:1.753863 loss_tea_ce:0.165064 loss_dis:0.115474 loss_sum:3.073667 loss:5.976623
Train epoch:12 batch:80 loss_stu_ce:0.128299 loss_tea_ce:2.234970 loss_dis:0.095606 loss_sum:3.319331 loss:5.976623
Train epoch:12 batch:120 loss_stu_ce:1.654856 loss_tea_ce:0.140176 loss_dis:0.114606 loss_sum:2.941087 loss:6.023303
Train epoch:12 batch:120 loss_stu_ce:0.117152 loss_tea_ce:2.087339 loss_dis:0.090780 loss_sum:3.112295 loss:6.023303
Train epoch:12 batch:160 loss_stu_ce:1.696848 loss_tea_ce:0.139205 loss_dis:0.114925 loss_sum:2.985305 loss:6.070885
Train epoch:12 batch:160 loss_stu_ce:0.140956 loss_tea_ce:2.087471 loss_dis:0.088126 loss_sum:3.109689 loss:6.070885
Valid epoch:12 loss:4.487773 acc@1:0.514943 acc@5:0.676178 macro_p:0.574033 macro_r:0.500015 macro_f1:0.518548
EarlyStopping counter: 1 out of 3
Train epoch:13 batch:0 loss_stu_ce:1.483742 loss_tea_ce:0.146616 loss_dis:0.108042 loss_sum:2.710779 loss:5.623418
Train epoch:13 batch:0 loss_stu_ce:0.145060 loss_tea_ce:1.933356 loss_dis:0.083422 loss_sum:2.912638 loss:5.623418
Train epoch:13 batch:40 loss_stu_ce:1.548992 loss_tea_ce:0.130120 loss_dis:0.113346 loss_sum:2.812568 loss:5.742307
Train epoch:13 batch:40 loss_stu_ce:0.126698 loss_tea_ce:1.924605 loss_dis:0.085855 loss_sum:2.909854 loss:5.742307
Train epoch:13 batch:80 loss_stu_ce:1.504557 loss_tea_ce:0.155605 loss_dis:0.110511 loss_sum:2.765275 loss:5.822742
Train epoch:13 batch:80 loss_stu_ce:0.133286 loss_tea_ce:1.833542 loss_dis:0.086306 loss_sum:2.829887 loss:5.822742
Train epoch:13 batch:120 loss_stu_ce:1.716797 loss_tea_ce:0.148985 loss_dis:0.114506 loss_sum:3.010840 loss:5.874620
Train epoch:13 batch:120 loss_stu_ce:0.131428 loss_tea_ce:2.102913 loss_dis:0.091332 loss_sum:3.147657 loss:5.874620
Train epoch:13 batch:160 loss_stu_ce:1.689359 loss_tea_ce:0.136296 loss_dis:0.116678 loss_sum:2.992437 loss:5.921040
Train epoch:13 batch:160 loss_stu_ce:0.118821 loss_tea_ce:2.215164 loss_dis:0.090112 loss_sum:3.235104 loss:5.921040
Valid epoch:13 loss:4.471714 acc@1:0.516050 acc@5:0.675194 macro_p:0.572050 macro_r:0.502242 macro_f1:0.519826
Validation loss decreased (4.476394 --> 4.471714).  Saving model ...
Train epoch:14 batch:0 loss_stu_ce:1.464866 loss_tea_ce:0.140997 loss_dis:0.109666 loss_sum:2.702527 loss:5.553056
Train epoch:14 batch:0 loss_stu_ce:0.128775 loss_tea_ce:1.909382 loss_dis:0.081237 loss_sum:2.850529 loss:5.553056
Train epoch:14 batch:40 loss_stu_ce:1.473651 loss_tea_ce:0.125587 loss_dis:0.108410 loss_sum:2.683341 loss:5.638761
Train epoch:14 batch:40 loss_stu_ce:0.121197 loss_tea_ce:1.864017 loss_dis:0.082574 loss_sum:2.810958 loss:5.638761
Train epoch:14 batch:80 loss_stu_ce:1.739343 loss_tea_ce:0.151704 loss_dis:0.118839 loss_sum:3.079437 loss:5.671344
Train epoch:14 batch:80 loss_stu_ce:0.119314 loss_tea_ce:2.116868 loss_dis:0.090967 loss_sum:3.145847 loss:5.671344
Train epoch:14 batch:120 loss_stu_ce:1.504015 loss_tea_ce:0.125844 loss_dis:0.111739 loss_sum:2.747250 loss:5.740590
Train epoch:14 batch:120 loss_stu_ce:0.114324 loss_tea_ce:1.804906 loss_dis:0.083222 loss_sum:2.751449 loss:5.740590
Train epoch:14 batch:160 loss_stu_ce:1.485894 loss_tea_ce:0.131394 loss_dis:0.108584 loss_sum:2.703125 loss:5.772964
Train epoch:14 batch:160 loss_stu_ce:0.117754 loss_tea_ce:1.936961 loss_dis:0.085647 loss_sum:2.911182 loss:5.772964
Valid epoch:14 loss:4.461123 acc@1:0.517034 acc@5:0.676629 macro_p:0.570857 macro_r:0.502747 macro_f1:0.519827
Validation loss decreased (4.471714 --> 4.461123).  Saving model ...
Train epoch:15 batch:0 loss_stu_ce:1.494893 loss_tea_ce:0.144551 loss_dis:0.107841 loss_sum:2.717856 loss:5.616393
Train epoch:15 batch:0 loss_stu_ce:0.133141 loss_tea_ce:1.954468 loss_dis:0.081093 loss_sum:2.898537 loss:5.616393
Train epoch:15 batch:40 loss_stu_ce:1.425757 loss_tea_ce:0.128448 loss_dis:0.111936 loss_sum:2.673565 loss:5.454357
Train epoch:15 batch:40 loss_stu_ce:0.123120 loss_tea_ce:1.815840 loss_dis:0.082709 loss_sum:2.766050 loss:5.454357
Train epoch:15 batch:80 loss_stu_ce:1.387910 loss_tea_ce:0.115087 loss_dis:0.112513 loss_sum:2.628122 loss:5.488789
Train epoch:15 batch:80 loss_stu_ce:0.117470 loss_tea_ce:1.717579 loss_dis:0.079084 loss_sum:2.625889 loss:5.488789
Train epoch:15 batch:120 loss_stu_ce:1.582193 loss_tea_ce:0.132391 loss_dis:0.113228 loss_sum:2.846861 loss:5.539292
Train epoch:15 batch:120 loss_stu_ce:0.121892 loss_tea_ce:1.994893 loss_dis:0.084646 loss_sum:2.963240 loss:5.539292
Train epoch:15 batch:160 loss_stu_ce:1.658771 loss_tea_ce:0.139637 loss_dis:0.116864 loss_sum:2.967048 loss:5.573366
Train epoch:15 batch:160 loss_stu_ce:0.137135 loss_tea_ce:1.975033 loss_dis:0.088763 loss_sum:2.999796 loss:5.573366
Valid epoch:15 loss:4.462809 acc@1:0.518632 acc@5:0.676588 macro_p:0.571132 macro_r:0.505433 macro_f1:0.521995
EarlyStopping counter: 1 out of 3
Train epoch:16 batch:0 loss_stu_ce:1.401055 loss_tea_ce:0.126387 loss_dis:0.105072 loss_sum:2.578161 loss:5.335978
Train epoch:16 batch:0 loss_stu_ce:0.120181 loss_tea_ce:1.853204 loss_dis:0.078443 loss_sum:2.757816 loss:5.335978
Train epoch:16 batch:40 loss_stu_ce:1.430580 loss_tea_ce:0.124921 loss_dis:0.109081 loss_sum:2.646307 loss:5.267450
Train epoch:16 batch:40 loss_stu_ce:0.126311 loss_tea_ce:1.859302 loss_dis:0.082088 loss_sum:2.806494 loss:5.267450
Train epoch:16 batch:80 loss_stu_ce:1.389838 loss_tea_ce:0.126693 loss_dis:0.110832 loss_sum:2.624853 loss:5.310082
Train epoch:16 batch:80 loss_stu_ce:0.125302 loss_tea_ce:1.719064 loss_dis:0.082032 loss_sum:2.664686 loss:5.310082
Train epoch:16 batch:120 loss_stu_ce:1.481541 loss_tea_ce:0.116668 loss_dis:0.115042 loss_sum:2.748624 loss:5.371715
Train epoch:16 batch:120 loss_stu_ce:0.118912 loss_tea_ce:1.782038 loss_dis:0.083143 loss_sum:2.732379 loss:5.371715
Train epoch:16 batch:160 loss_stu_ce:1.510278 loss_tea_ce:0.128498 loss_dis:0.110344 loss_sum:2.742212 loss:5.432320
Train epoch:16 batch:160 loss_stu_ce:0.123116 loss_tea_ce:1.942242 loss_dis:0.085641 loss_sum:2.921771 loss:5.432320
Valid epoch:16 loss:4.476751 acc@1:0.519534 acc@5:0.678760 macro_p:0.570518 macro_r:0.506009 macro_f1:0.522436
EarlyStopping counter: 2 out of 3
Train epoch:17 batch:0 loss_stu_ce:1.408771 loss_tea_ce:0.120877 loss_dis:0.108726 loss_sum:2.616913 loss:5.323741
Train epoch:17 batch:0 loss_stu_ce:0.103997 loss_tea_ce:1.807068 loss_dis:0.079576 loss_sum:2.706828 loss:5.323741
Train epoch:17 batch:40 loss_stu_ce:1.315009 loss_tea_ce:0.115849 loss_dis:0.106513 loss_sum:2.495984 loss:5.222979
Train epoch:17 batch:40 loss_stu_ce:0.099418 loss_tea_ce:1.729117 loss_dis:0.078713 loss_sum:2.615667 loss:5.222979
Train epoch:17 batch:80 loss_stu_ce:1.250794 loss_tea_ce:0.118285 loss_dis:0.108106 loss_sum:2.450142 loss:5.216063
Train epoch:17 batch:80 loss_stu_ce:0.109106 loss_tea_ce:1.616658 loss_dis:0.077405 loss_sum:2.499817 loss:5.216063
Train epoch:17 batch:120 loss_stu_ce:1.306461 loss_tea_ce:0.126369 loss_dis:0.105217 loss_sum:2.484997 loss:5.260144
Train epoch:17 batch:120 loss_stu_ce:0.115908 loss_tea_ce:1.682755 loss_dis:0.076835 loss_sum:2.567008 loss:5.260144
Train epoch:17 batch:160 loss_stu_ce:1.351694 loss_tea_ce:0.113371 loss_dis:0.107937 loss_sum:2.544438 loss:5.321530
Train epoch:17 batch:160 loss_stu_ce:0.101864 loss_tea_ce:1.727187 loss_dis:0.080870 loss_sum:2.637748 loss:5.321530
Valid epoch:17 loss:4.478105 acc@1:0.519452 acc@5:0.677571 macro_p:0.569850 macro_r:0.505983 macro_f1:0.522120
EarlyStopping counter: 3 out of 3
Early Stop!
Loaded backend agg version unknown.
Test 	 loss:3.592713 acc@1:0.418952 acc@5:0.555084 macro_p:0.466318 macro_r:0.405825 macro_f1:0.416173
Test 	 loss:3.871286 acc@1:0.391855 acc@5:0.506005 macro_p:0.439270 macro_r:0.382279 macro_f1:0.389398
Total time elapsed: 2470.2035s
Fininsh trainning in seed 666

